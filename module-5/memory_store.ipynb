{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xmualex2023/langchain-project/blob/main/module-5/memory_store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYomgglt2SL"
      },
      "source": [
        "# Chatbot with Memory\n",
        "\n",
        "## Review\n",
        "\n",
        "[Memory](https://pmc.ncbi.nlm.nih.gov/articles/PMC10410470/) is a cognitive function that allows people to store, retrieve, and use information to understand their present and future.\n",
        "\n",
        "There are [various long-term memory types](https://langchain-ai.github.io/langgraph/concepts/memory/#memory) that can be used in AI applications.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Here, we'll introduce the [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) as a way to save and retrieve long-term memories.\n",
        "\n",
        "We'll build a chatbot that uses both `short-term (within-thread)` and `long-term (across-thread)` memory.\n",
        "\n",
        "We'll focus on long-term [semantic memory](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory), which will be facts about the user.\n",
        "\n",
        "These long-term memories will be used to create a personalized chatbot that can remember facts about the user.\n",
        "\n",
        "It will save memory [\"in the hot path\"](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories), as the user is chatting with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AHvIizeEt2SN"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U google-generativeai langchain-google-genai  langgraph langchain_core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE_ipDpft2SO"
      },
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DoTrpQTrt2SP",
        "outputId": "82fbc981-cc1e-4d76-8f92-54f02b352a2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LANGCHAIN_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grHu6Agwt2SP"
      },
      "source": [
        "## Introduction to the LangGraph Store\n",
        "\n",
        "The [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) provides a way to store and retrieve information *across threads* in LangGraph.\n",
        "\n",
        "This is an  [open source base class](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/) for persistent `key-value` stores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7q8BeMKUt2SP"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "in_memory_store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xypdlaYt2SQ"
      },
      "source": [
        "When storing objects (e.g., memories) in the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore), we provide:\n",
        "\n",
        "- The `namespace` for the object, a tuple (similar to directories)\n",
        "- the object `key` (similar to filenames)\n",
        "- the object `value` (similar to file contents)\n",
        "\n",
        "We use the [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) method to save an object to the store by `namespace` and `key`.\n",
        "\n",
        "![langgraph_store.png](attachment:6281b4e3-4930-467e-83ce-ba1aa837ca16.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vqcDT4-ut2SQ"
      },
      "outputs": [],
      "source": [
        "# Namespace for the memory to save\n",
        "user_id = \"1\"\n",
        "namespace_for_memory = (user_id, \"memories\")\n",
        "\n",
        "# Save a memory to namespace as key and value\n",
        "key = str(uuid.uuid4())\n",
        "\n",
        "# The value needs to be a dictionary\n",
        "value = {\"food_preference\" : \"I like pizza\"}\n",
        "\n",
        "# Save the memory\n",
        "in_memory_store.put(namespace_for_memory, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUVV2juZt2SQ"
      },
      "source": [
        "We use [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) to retrieve objects from the store by `namespace`.\n",
        "\n",
        "This returns a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XGZsar_9t2SQ",
        "outputId": "eaebb930-eba2-495b-efaf-e971187a75ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Search\n",
        "memories = in_memory_store.search(namespace_for_memory)\n",
        "type(memories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Di2oM8HDt2SR",
        "outputId": "c6e559ac-5f84-42f9-b138-062f09e2b71f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'namespace': ['1', 'memories'],\n",
              " 'key': '6523cb77-4a74-4d90-8ab9-7e07c55cc0b7',\n",
              " 'value': {'food_preference': 'I like pizza'},\n",
              " 'created_at': '2025-01-24T11:09:52.122591+00:00',\n",
              " 'updated_at': '2025-01-24T11:09:52.122598+00:00',\n",
              " 'score': None}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Metatdata\n",
        "memories[0].dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZR_xoamIt2SS",
        "outputId": "251a7deb-b9aa-485e-dcbe-312c3868b7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6523cb77-4a74-4d90-8ab9-7e07c55cc0b7 {'food_preference': 'I like pizza'}\n"
          ]
        }
      ],
      "source": [
        "# The key, value\n",
        "print(memories[0].key, memories[0].value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7wJHWBot2SS"
      },
      "source": [
        "We can also use [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) to retrieve an object by `namespace` and `key`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "viPSOiHkt2SS",
        "outputId": "3f7bd5f0-640d-437c-8033-b18206d57030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'namespace': ['1', 'memories'],\n",
              " 'key': '6523cb77-4a74-4d90-8ab9-7e07c55cc0b7',\n",
              " 'value': {'food_preference': 'I like pizza'},\n",
              " 'created_at': '2025-01-24T11:09:52.122591+00:00',\n",
              " 'updated_at': '2025-01-24T11:09:52.122598+00:00'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Get the memory by namespace and key\n",
        "memory = in_memory_store.get(namespace_for_memory, key)\n",
        "memory.dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7PAJdt5t2SS"
      },
      "source": [
        "## Chatbot with long-term memory\n",
        "\n",
        "We want a chatbot that [has two types of memory](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_156):\n",
        "\n",
        "1. `Short-term (within-thread) memory`: Chatbot can persist conversational history and / or allow interruptions in a chat session.\n",
        "2. `Long-term (cross-thread) memory`: Chatbot can remember information about a specific user *across all chat sessions*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w-u7Mk9ht2ST",
        "outputId": "25587930-0620-4a20-e106-5ef1bbbdba5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GOOGLE_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "# _set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwZ14epVt2ST"
      },
      "source": [
        "For `short-term memory`, we'll use a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries).\n",
        "\n",
        "See Module 2 and our [conceptual docs](https://langchain-ai.github.io/langgraph/concepts/persistence/) for more on checkpointers, but in summary:\n",
        "\n",
        "* They write the graph state at each step to a thread.\n",
        "* They persist the chat history in the thread.\n",
        "* They allow the graph to be interrupted and / or resumed from any step in the thread.\n",
        "\n",
        "And, for `long-term memory`, we'll use the [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) as introduced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d4jJtISAt2ST"
      },
      "outputs": [],
      "source": [
        "# Chat model\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize the LLM\n",
        "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# 初始化 Gemini 模型\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    # convert_system_message_to_human=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mODQxc3jt2ST"
      },
      "source": [
        "The chat history will be saved to short-term memory using the checkpointer.\n",
        "\n",
        "The chatbot will reflect on the chat history.\n",
        "\n",
        "It will then create and save a memory to the [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore).\n",
        "\n",
        "This memory is accessible in future chat sessions to personalize the chatbot's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "26fgzOLxt2ST",
        "outputId": "1182bfb7-6fbe-4437-f42a-436726876b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.store.base import BaseStore\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "\n",
        "# Chatbot instruction\n",
        "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user.\n",
        "If you have memory for this user, use it to personalize your responses.\n",
        "Here is the memory (it may be empty): {memory}\"\"\"\n",
        "\n",
        "# Create new memory from the chat history and any existing memory\n",
        "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
        "\n",
        "CURRENT USER INFORMATION:\n",
        "{memory}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the chat history below carefully\n",
        "2. Identify new information about the user, such as:\n",
        "   - Personal details (name, location)\n",
        "   - Preferences (likes, dislikes)\n",
        "   - Interests and hobbies\n",
        "   - Past experiences\n",
        "   - Goals or future plans\n",
        "3. Merge any new information with existing memory\n",
        "4. Format the memory as a clear, bulleted list\n",
        "5. If new information conflicts with existing memory, keep the most recent version\n",
        "\n",
        "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
        "\n",
        "Based on the chat history below, please update the user information:\"\"\"\n",
        "\n",
        "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
        "\n",
        "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
        "\n",
        "    # Get the user ID from the config\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # Retrieve memory from the store\n",
        "    namespace = (\"memory\", user_id)\n",
        "    key = \"user_memory\"\n",
        "    existing_memory = store.get(namespace, key)\n",
        "\n",
        "    # Extract the actual memory content if it exists and add a prefix\n",
        "    if existing_memory:\n",
        "        # Value is a dictionary with a memory key\n",
        "        existing_memory_content = existing_memory.value.get('memory')\n",
        "    else:\n",
        "        existing_memory_content = \"No existing memory found.\"\n",
        "\n",
        "    # Format the memory in the system prompt\n",
        "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
        "\n",
        "    # Respond using memory as well as the chat history\n",
        "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
        "\n",
        "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
        "\n",
        "    # Get the user ID from the config\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # Retrieve existing memory from the store\n",
        "    namespace = (\"memory\", user_id)\n",
        "    existing_memory = store.get(namespace, \"user_memory\")\n",
        "\n",
        "    # Extract the memory\n",
        "    if existing_memory:\n",
        "        existing_memory_content = existing_memory.value.get('memory')\n",
        "    else:\n",
        "        existing_memory_content = \"No existing memory found.\"\n",
        "\n",
        "    # Format the memory in the system prompt\n",
        "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
        "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
        "\n",
        "    # Overwrite the existing memory in the store\n",
        "    key = \"user_memory\"\n",
        "\n",
        "    # Write value as a dictionary with a memory key\n",
        "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
        "\n",
        "# Define the graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"call_model\", call_model)\n",
        "builder.add_node(\"write_memory\", write_memory)\n",
        "builder.add_edge(START, \"call_model\")\n",
        "builder.add_edge(\"call_model\", \"write_memory\")\n",
        "builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "# Store for long-term (across-thread) memory\n",
        "across_thread_memory = InMemoryStore()\n",
        "\n",
        "# Checkpointer for short-term (within-thread) memory\n",
        "within_thread_memory = MemorySaver()\n",
        "\n",
        "# Compile the graph with the checkpointer fir and store\n",
        "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj67g6A_t2ST"
      },
      "source": [
        "When we interact with the chatbot, we supply two things:\n",
        "\n",
        "1. `Short-term (within-thread) memory`: A `thread ID` for persisting the chat history.\n",
        "2. `Long-term (cross-thread) memory`: A `user ID` to namespace long-term memories to the user.\n",
        "\n",
        "Let's see how these work together in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AUl4GQ1it2SU",
        "outputId": "6f491e36-04f9-4144-be01-c99852c7d0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi, my name is Lance\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Lance! It's nice to meet you.  How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "# We supply a thread ID for short-term (within-thread) memory\n",
        "# We supply a user ID for long-term (across-thread) memory\n",
        "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
        "\n",
        "# User input\n",
        "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0yrBIta7t2SU",
        "outputId": "5626cd7d-a510-4037-fcd3-29d166e7927c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I like to bike around San Francisco\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's great, Lance! San Francisco is a fantastic city for biking, with so many diverse routes and beautiful scenery.  Do you have any particular areas you enjoy biking in, or types of rides you prefer (e.g., long distance, hilly, scenic)?  Knowing that would help me give you more specific recommendations.\n"
          ]
        }
      ],
      "source": [
        "# User input\n",
        "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Kyx7fut2SU"
      },
      "source": [
        "We're using the `MemorySaver` checkpointer for within-thread memory.\n",
        "\n",
        "This saves the chat history to the thread.\n",
        "\n",
        "We can look at the chat history saved to the thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n1FgFvllt2SU",
        "outputId": "117d0191-4fca-435a-961f-c97a75dbd699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi, my name is Lance\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Lance! It's nice to meet you.  How can I help you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I like to bike around San Francisco\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's great, Lance! San Francisco is a fantastic city for biking, with so many diverse routes and beautiful scenery.  Do you have any particular areas you enjoy biking in, or types of rides you prefer (e.g., long distance, hilly, scenic)?  Knowing that would help me give you more specific recommendations.\n"
          ]
        }
      ],
      "source": [
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "state = graph.get_state(thread).values\n",
        "for m in state[\"messages\"]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m10NsSPt2SU"
      },
      "source": [
        "Recall that we compiled the graph with our the store:\n",
        "\n",
        "```python\n",
        "across_thread_memory = InMemoryStore()\n",
        "```\n",
        "\n",
        "And, we added a node to the graph (`write_memory`) that reflects on the chat history and saves a memory to the store.\n",
        "\n",
        "We can to see if the memory was saved to the store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9nfUunYZt2SU",
        "outputId": "d2d03f3e-f1d7-43f9-8f21-3d61306fe47c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'namespace': ['memory', '1'],\n",
              " 'key': 'user_memory',\n",
              " 'value': {'memory': \"Here's an updated summary of what I know about you:\\n\\n* **Name:** Lance\\n* **Location:** San Francisco\\n* **Interests/Hobbies:** Biking\"},\n",
              " 'created_at': '2025-01-24T11:14:08.377077+00:00',\n",
              " 'updated_at': '2025-01-24T11:14:08.377080+00:00'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Namespace for the memory to save\n",
        "user_id = \"1\"\n",
        "namespace = (\"memory\", user_id)\n",
        "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
        "existing_memory.dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnnzGKsjt2SU"
      },
      "source": [
        "Now, let's kick off a *new thread* with the *same user ID*.\n",
        "\n",
        "We should see that the chatbot remembered the user's profile and used it to personalize the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6OSa2-BEt2SU",
        "outputId": "39d33ae5-0d22-45b2-dc0e-640404c1089d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi! Where would you recommend that I go biking?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Lance!  Since you're in San Francisco and enjoy biking, I have a few recommendations depending on what kind of ride you're looking for:\n",
            "\n",
            "For stunning views:\n",
            "\n",
            "* **Golden Gate Bridge Bike Trail:** This is a classic for a reason!  The views are incredible, and you can choose to bike across the bridge and back, or continue further into Marin County.  Just be aware of the potential for wind and crowds.\n",
            "\n",
            "For a challenging ride:\n",
            "\n",
            "* **Mount Tamalpais:**  This offers a serious climb with rewarding panoramic views at the top.  There are various trails to choose from, so you can pick one that suits your fitness level.\n",
            "\n",
            "For a more relaxed ride:\n",
            "\n",
            "* **Presidio Promenade Trail:** This paved trail offers beautiful views of the Golden Gate Bridge and Alcatraz, with relatively flat terrain.  It's perfect for a leisurely ride.\n",
            "* **Lake Merced Loop:** A mostly flat, scenic loop around Lake Merced, great for a casual ride.\n",
            "\n",
            "To give you the best recommendations, could you tell me what kind of biking you prefer (e.g., road biking, mountain biking, leisurely rides)?  Knowing your preferred difficulty level would also be helpful.\n"
          ]
        }
      ],
      "source": [
        "# We supply a user ID for across-thread memory as well as a new thread ID\n",
        "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
        "\n",
        "# User input\n",
        "input_messages = [HumanMessage(content=\"Hi! Where would you recommend that I go biking?\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IMxhp3o9t2SV",
        "outputId": "0e2db888-aa4a-483e-846b-d7650e4244a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Great, are there any bakeries nearby that I can check out? I like a croissant after biking.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That sounds like a perfect post-bike reward!  Since I don't know your exact location after your bike ride, I can't give you hyper-local recommendations.  However, to help me narrow it down, could you tell me which area of San Francisco you'll be biking in?  \n",
            "\n",
            "Knowing the neighborhood will allow me to suggest some excellent bakeries nearby.  For example, if you're near the Golden Gate Bridge, I can recommend places in Sausalito or the Presidio.  If you're in the Mission District, I have some fantastic local favorites to suggest.\n"
          ]
        }
      ],
      "source": [
        "# User input\n",
        "input_messages = [HumanMessage(content=\"Great, are there any bakeries nearby that I can check out? I like a croissant after biking.\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo-KhOAjt2SV"
      },
      "source": [
        "## Viewing traces in LangSmith\n",
        "\n",
        "We can see that the memories are retrieved from the store and supplied as part of the system prompt, as expected:\n",
        "\n",
        "https://smith.langchain.com/public/10268d64-82ff-434e-ac02-4afa5cc15432/r\n",
        "\n",
        "## Studio\n",
        "\n",
        "We can also interact with our chatbot in Studio.\n",
        "\n",
        "![Screenshot 2024-10-28 at 10.08.27 AM.png](attachment:afa216f7-4b67-4783-82af-c319e0f512ac.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN6bjV0bt2SV"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}